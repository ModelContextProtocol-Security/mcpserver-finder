---
title: [Check Name] - TEMPLATE
version: 1.0
date: [YYYY-MM-DD]
tags: [quality, assessment, MCP, [specific-area]]
aliases: [[alternative-names]]
status: template
---

# [Check Name] - Quality Assessment Check

## Assessment Metadata

**Confidence Level**: High/Medium/Low/Variable
**Complexity**: Simple/Moderate/Complex/Expert  
**Data Requirements**: Always/Usually/Sometimes/Rarely Available
**Time to Complete**: X minutes (automated) + Y minutes (manual)
**Reliability Notes**: [What factors affect confidence in this assessment]

## Purpose & Context

[Explain why this quality aspect matters for MCP server selection]

**Decision Impact**: [How this assessment influences server choice - High/Medium/Low impact]
**When to Use**: [Specific scenarios where this check is most valuable]
**When to Skip**: [Situations where this check might not be relevant]

## Assessment Criteria

### High Confidence Indicators
- **Green Flags**: [Specific, objective indicators of quality]
- **Quantitative Thresholds**: [Specific numbers/metrics that indicate good quality]
- **Clear Patterns**: [Obvious signs this aspect is well-handled]

### Medium Confidence Indicators  
- **Positive Patterns**: [Patterns that suggest quality but require interpretation]
- **Contextual Signals**: [Indicators that depend on project context]
- **Trend Analysis**: [Patterns over time that suggest quality]

### Low Confidence / Subjective Areas
- **Expert Judgment Required**: [Areas needing significant experience to assess]
- **Context-Dependent**: [Factors that vary greatly by use case]
- **Interpretation Needed**: [Signals that require domain knowledge]

## Automated Assessment (For AI Assistants)

### High Confidence Checks
```bash
# [Specific commands, API calls, or scripts]
# [Clear thresholds for success/failure]
# [Objective metrics extraction]
```

### Pattern Recognition Checks  
```bash
# [Commands that require interpretation]
# [Guidance for pattern analysis]
# [Context clues to look for]
```

**Automation Reliability**: [How much to trust automated results vs. human verification]

## Manual Assessment (For Humans)

### Step-by-Step Checklist
- [ ] **[Objective Check]**: [Specific thing to verify - High confidence]
- [ ] **[Semi-Objective Check]**: [Pattern to look for - Medium confidence]  
- [ ] **[Subjective Check]**: [Judgment call required - Low confidence]
- [ ] **[Context Check]**: [Situational assessment - Variable confidence]

### Key Questions to Ask
- **High Confidence**: "[Objective question with clear answer]"
- **Medium Confidence**: "[Question requiring some interpretation]"  
- **Low Confidence**: "[Question requiring expert judgment]"

### Human Verification Points
- [When to double-check automated results]
- [Subjective assessments that can't be automated]
- [Context-specific considerations]

## Good vs. Bad Patterns

### Excellent Quality Examples
**Pattern**: [Specific observable pattern]
**Why Good**: [Explanation of why this indicates quality]
**Confidence**: High/Medium/Low in identifying this pattern
**Example**: [Real-world example if possible]

### Poor Quality Examples  
**Pattern**: [Specific observable pattern]
**Why Bad**: [Explanation of why this indicates problems]
**Confidence**: High/Medium/Low in identifying this pattern
**Example**: [Real-world example if possible]

### Ambiguous Cases
**Pattern**: [Patterns that could be good or bad depending on context]
**Context Factors**: [What determines if this is good or bad]
**Assessment Approach**: [How to evaluate these ambiguous cases]

## Confidence Indicators

### High Confidence Results
**When to Trust**: [Conditions where this assessment is highly reliable]
**Minimum Data**: [Minimum information needed for reliable assessment]
**Strong Signals**: [Clear indicators that increase confidence]

### Low Confidence Warnings
**Red Flags for Assessment**: [When to be skeptical of results]
**Data Quality Issues**: [Problems that undermine assessment reliability]
**External Factors**: [Things outside the check that affect reliability]

### Improving Confidence
**Additional Data**: [What extra information would improve reliability]
**Cross-Validation**: [Other checks that can confirm/contradict findings]
**Time Factors**: [How time/maturity affects assessment confidence]

## Justification & Evidence

### Why This Matters
**Impact on Quality**: [How this factor affects overall server quality]
**Risk Implications**: [What problems arise when this area is weak]
**User Experience**: [How this affects users of the server]

### Supporting Evidence
**Research Basis**: [Academic/industry research supporting this assessment]
**Best Practices**: [Industry standards or guidelines]
**Real-World Examples**: [Evidence from actual server projects]

### Decision Weighting
**Critical Factor**: [When this assessment should heavily influence decisions]
**Supporting Factor**: [When this is important but not decisive]  
**Context Dependent**: [When importance varies by situation]

## Integration Guidance

### Using High Confidence Results
- [How to incorporate reliable findings into selection decisions]
- [Weight to give these results vs. other assessments]
- [Clear decision rules based on confident findings]

### Handling Low Confidence Results
- [How to treat uncertain assessments]
- [When to seek additional validation]
- [How to communicate uncertainty to users]

### Combining with Other Checks
- [Which other checks complement this one]
- [How to resolve conflicts between different assessments]
- [Holistic evaluation approaches]

## Common Pitfalls

### Assessment Errors
- [Common mistakes in applying this check]
- [Misinterpretations to avoid]
- [Biases that can affect judgment]

### Context Mismatches
- [When this check doesn't apply well]
- [Server types where this is less relevant]  
- [Use cases that change the assessment criteria]

### Confidence Misjudgments
- [When people over-trust uncertain results]
- [When people under-value reliable findings]
- [How to calibrate confidence appropriately]

---

**Template Usage Notes:**
- Replace all [bracketed items] with specific content
- Choose appropriate confidence/complexity levels
- Include real examples where possible
- Focus on observable, actionable criteria
- Emphasize teaching over just checking